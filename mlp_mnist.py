# -*- coding: utf-8 -*-
"""Intro to CNNs

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WJSXdG9EvAg6zJ9M6yG6ZM9yRaIFmBAE
"""

from torchvision.transforms import transforms
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F
import torch.optim as optim
from torch import nn
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
from torch.utils.data import Dataset
import pandas as pd
import torch
import numpy as np

"""# PyTorch Core Functionality
PyTorch is a python package built by Facebook that provides two high-level features: 
1. **Tensor** computation (like Numpy) with strong GPU acceleration and 
2. **Deep Neural Networks** built on a tape-based automatic differentiation system.

For now, let's focus our attention on the word "*Tensor*". If you have ever taken an algebra course, there is a good chance that you know the following terms:
* **Scalar**: a non-dimensional value (zero-order)
* **Vector**: a one-dimensional list of values (first-order) 
* **Matrix**: a two-dimensional grid of values (second-order)

A **Tensor** is only a generalization of the above. Tensors are of $n^{th}$-order. By definition, scalars, vectors and matrices are also considered tensors. The following figure does a great job in simply conveying the idea of tensors:

![](https://images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com/f/b08f5de3-afa1-44fb-8902-d7845a3e79bc/ddyw3rm-5b02c848-c0ab-4ad8-883d-9236b0f209e2.png/v1/fill/w_1280,h_421,q_80,strp/screen_shot_2020_06_07_at_2_12_32_am_by_imadedd_ddyw3rm-fullview.jpg?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ1cm46YXBwOiIsImlzcyI6InVybjphcHA6Iiwib2JqIjpbW3siaGVpZ2h0IjoiPD00MjEiLCJwYXRoIjoiXC9mXC9iMDhmNWRlMy1hZmExLTQ0ZmItODkwMi1kNzg0NWEzZTc5YmNcL2RkeXczcm0tNWIwMmM4NDgtYzBhYi00YWQ4LTg4M2QtOTIzNmIwZjIwOWUyLnBuZyIsIndpZHRoIjoiPD0xMjgwIn1dXSwiYXVkIjpbInVybjpzZXJ2aWNlOmltYWdlLm9wZXJhdGlvbnMiXX0.pY6K5AhMgn6aVQqCojEeDuf9J2BnWInGY-GfNSy2YCg)
"""

a = list(range(10))
a

torch.tensor(a)


# creating a tensor from a Python list
torch.tensor(range(10))
# creating a tensor of radom values between 0 and 1
r = torch.rand((5, 5))
# creating a tensor of zeros
z = torch.zeros((4, 5))
# creating a tensor of ones
o = torch.ones((5, 5))
t = torch.ones(5) * 0.4
# tensor multiplication
2 * o + t
# tensor element-wise addition
x = r + o

# reshaping the tensor
x = torch.rand([5, 6])
x.view((15, 2))

"""# Data in PyTorch

1. Raw Data
This is typically your starting point. Raw data may be a numpy array, a pandas dataframe, a Python dictionary, 
a list of Pillow images, or other data structures.

For our purpose, we will use dataset from a `CSV` file already preloaded in 
Google's Colab in the path `/content/sample_data/mnist_train_small.csv`.
"""

# loading MNIST from a csv, we will use pandas to do so.
df_train = pd.read_csv('mnist_train_small.csv', header=None)  # training data
df_test = pd.read_csv('mnist_test.csv', header=None)  # testing data
df_train

df_train.describe()

"""Let's step back for a minute to talk about this `CSV` file. It might be a 
little unorthodox to load a dataset of images from a CSV file. 
How is that even possible? Well, notice how the first column is a value between 0 and 9. 
This is, in fact, our class. What about every other column? Since our dataset is a list of 28x28 images,
we can simply store the total 784 pixel values in a single row (vector).

This process is called flattenning the images.

Lastly, let's convert our data into a numpy array, then split it into images and labels.
"""

data_train = df_train.to_numpy()
labels_train = data_train[:, 0]  # the accurate classification
images_train = data_train[:, 1:]  # the pixels (flattened image)

labels_train.shape, images_train.shape

data_test = df_test.to_numpy()
labels_test = data_test[:, 0]  # the accurate classification
images_test = data_test[:, 1:]  # the pixels (flattened image)

labels_train.shape, images_train.shape

data_train.shape

# count data distribution
for i in range(10):
    s = np.sum(labels_train == i)
    print(f"{i}: {s}")

"""
2. `torch.utils.data.`**`Dataset`**
In order to standardize this raw data to be used in training, we must implement the `torch.utils.data.Dataset` 
abstract class based on the nature of our data. The subclass that is created must override the following functions:
  - `__len__` so that len(dataset) returns the size of the dataset.
  - `__getitem__` to support the indexing such that `dataset[i]` can be used to get $i^{th}$ sample.
"""


class MNISTDataset(Dataset):
    def __init__(self, images, labels):
        self.images = images / 255
        self.labels = labels

    def __getitem__(self, idx):
        x = self.images[idx]
        y = self.labels[idx]
        return torch.tensor(x), torch.tensor(y)

    def __len__(self):
        return self.images.shape[0]


# Instantiating the MNISTDataset
trainset = MNISTDataset(images_train, labels_train)
testset = MNISTDataset(images_test, labels_test)

# Visualizing a data sample

x, y = testset[0]
x = x.numpy() * 255
x = x.reshape((28, 28))
plt.imshow(x)

"""## 3. `torch.utils.data.`**`Dataloader`**"""
batch_size = 100

# Creates an iterable object that iterates over batches of specific batch size.
trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)
testloader = DataLoader(testset, batch_size=batch_size)

"""# Building the network!

For the sake of this demonstration, let us start with a fully connected neural network (or a multi-layer perceptron). 

The network we will be building in PyTorch is similar to the one demonstrated above. 
We will have 3 layers (the input layer does not usually count):

- Two hidden layers with 1000 hidden nodes and "Sigmoid" activation function. 
- A final output layer with "Softmax activation" with 10 output nodes for each digit.
"""


# Designing the model

class FullyConnected(torch.nn.Module):
    def __init__(self, input_size, output_size):
        super(FullyConnected, self).__init__()

        # First hidden layer
        self.fc1 = nn.Linear(input_size, 1000)
        self.sigmoid = nn.Sigmoid()

        # Second hidden layer
        self.fc2 = nn.Linear(1000, 1000)

        # Output layer
        self.fc3 = nn.Linear(1000, output_size)
        self.softmax = nn.Softmax()

    def forward(self, x):
        x = x.float()
        x = self.fc1(x)
        x = self.sigmoid(x)
        x = self.fc2(x)
        x = self.sigmoid(x)
        x = self.fc3(x)
        output = self.softmax(x)
        return output


device = 'cuda' if torch.cuda.is_available() else 'cpu'


model = FullyConnected(784, 10)
model.to(device)


lossfun = nn.CrossEntropyLoss()
lossfun.to(device)

optimizer = optim.Adam(model.parameters(), lr=0.001)


num_epochs = 10
print_every = 100

for epoch in range(num_epochs):  # loop over the testdata
    running_loss = 0.0
    running_acc = 0.0

    for i, data in enumerate(trainloader, 0):
        # get the inputs; data is a list of [inputs, labels]
        inputs, labels = data

        # move the data to GPU
        inputs = inputs.to(device)
        labels = labels.to(device)

        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize
        outputs = model(inputs)

        loss = lossfun(outputs, labels)
        loss.backward()

        optimizer.step()

        running_acc += torch.sum(outputs.argmax(dim=1)
                                 == labels).item() / batch_size

        # print statistics
        running_loss += loss.item()
        if i % print_every == print_every-1:
            with torch.no_grad():
                val_acc = []
                for i, data in enumerate(trainloader, 0):
                    # get the inputs; data is a list of [inputs, labels]
                    inputs, labels = data

                    # move the data to GPU
                    inputs = inputs.to(device)
                    labels = labels.to(device)

                    val_acc.append(
                        torch.sum(outputs.argmax(dim=1) == labels).item() / batch_size)

                val_acc = sum(val_acc) / len(val_acc)

            print('[%d, %5d] loss: %.3f - acc: %.3f - val acc: %.3f' %
                  (epoch + 1, i + 1, running_loss / print_every, running_acc * 100 / print_every, val_acc))

            running_loss = 0.0
            running_acc = 0.0

print('Finished Training')
count_right = 0
count_wrong = 0
total = 0

for i, data in enumerate(testloader, 0):
    # get the inputs; data is a list of [inputs, labels]
    inputs, labels = data

    # move the data to GPU
    inputs = inputs.to(device)
    # labels = labels.to(device)

    # get a guess
    outputs = model(inputs)

    outputs = outputs.detach().cpu().numpy()
    labels = labels.numpy()

    for j in range(len(outputs)):

        guess = np.where(outputs[j] == np.amax(outputs[j]))[0][0]
        correct = labels[j]
        accurate = guess == correct
        if guess == correct:
            count_right += 1
            # if total % 1000 < 3:
            #     print('Our Guess: {}, Correct Classification: {}\n'.format(guess, correct))
            #     print(outputs[j])
        # else:
        #     count_wrong += 1
        #     if count_wrong % 100 == 0:
        #         print('Our Guess: {}, Correct Classification: {}\n'.format(guess, correct))
        #         print(outputs[j])
        # print('Our Guess: {}, Correct Classification: {}\n'.format(guess, correct), accurate)
        total += 1

print('WE got {} out of {} ({}%)'.format(
    count_right, total, 100 * count_right / total))
